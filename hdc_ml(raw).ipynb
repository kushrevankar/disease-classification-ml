{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy scikit-learn iterative-stratification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1-DySsSnOI-",
        "outputId": "4fe6d6c3-c2d6-4584-d8c0-45857f9ecd2e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, iterative-stratification\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed iterative-stratification-0.1.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv40CASPoPuF",
        "outputId": "e480b934-d714-4e02-af7a-076291c81662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "cTaF4PqrnUEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/diseasedataset/finaldataset.csv\"\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "13Y4hINrouRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode diseases column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(df[[\"diseases\"]])\n",
        "\n",
        "# split into features & labels\n",
        "X = df.drop(columns=[\"diseases\"]).values\n",
        "y = y_encoded"
      ],
      "metadata": {
        "id": "NmTjPKPwo65o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-dtzem6mbpX",
        "outputId": "fe0786eb-188f-47ca-e6e0-bbd4da08a89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 168499 entries, 0 to 168498\n",
            "Columns: 379 entries, Unnamed: 0 to neck weakness\n",
            "dtypes: int64(378), object(1)\n",
            "memory usage: 487.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# splitting into train-test (one iteration only)\n",
        "for train_index, test_index in mskf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    break  # only take the first split"
      ],
      "metadata": {
        "id": "X6nEbuJ6pIzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HDCClassifier:\n",
        "    def __init__(self, dim=1000):\n",
        "        self.dim = dim  # Hyperdimensional vector size\n",
        "        self.prototypes = {}  # Stores prototype vectors per class\n",
        "\n",
        "    def _encode(self, x):\n",
        "        \"\"\" Convert input features into hyperdimensional binary vectors. \"\"\"\n",
        "        return torch.sign(torch.tensor(x, dtype=torch.float32))  # Convert to float\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        \"\"\" Train by averaging class-specific hyperdimensional vectors. \"\"\"\n",
        "        for label in np.unique(y_train):\n",
        "            class_vectors = [self._encode(X_train[i]) for i in range(len(y_train)) if y_train[i] == label]\n",
        "            self.prototypes[label] = torch.mean(torch.stack(class_vectors), dim=0).float()  # Ensure float type\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\" Predict based on similarity to stored class prototypes. \"\"\"\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            encoded_x = self._encode(x)\n",
        "            similarities = {label: torch.dot(encoded_x, proto) for label, proto in self.prototypes.items()}\n",
        "            predictions.append(max(similarities, key=similarities.get))  # Closest class\n",
        "        return np.array(predictions)"
      ],
      "metadata": {
        "id": "WIJh4DoHpe2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_labels = y_train.argmax(axis=1)  # Convert from one-hot to single-label vector\n",
        "y_test_labels = y_test.argmax(axis=1)"
      ],
      "metadata": {
        "id": "OeLHd7jnxUy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize & train\n",
        "model = HDCClassifier()\n",
        "model.fit(X_train, y_train_labels)"
      ],
      "metadata": {
        "id": "UNviPAwsppNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"hdc_model.pth\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"hdc_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8KZmbjRG73vn",
        "outputId": "c47d38fb-0df1-497f-d913-cc78a5740cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ca151639-82c1-4439-94e7-6caf924645b7\", \"hdc_model.pth\", 1375546)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "XxqIayAIprK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test_labels, y_pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "panS3nMhqLPh",
        "outputId": "58ae1dae-eab0-4895-8a2d-67f56431ee47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.871572700296736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_test_labels, y_pred, average='macro', zero_division=1)\n",
        "print(\"Precision:\", precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zdojMCusHNO",
        "outputId": "36898da7-5835-4939-cab1-759d6f3df15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8763759661465015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall = recall_score(y_test_labels, y_pred, average='macro', zero_division=1)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23_NC5U33Vvb",
        "outputId": "cea3232b-2ebe-460d-8a20-0c528a4c7ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.8781113494952544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clssf = classification_report(y_test_labels, y_pred)\n",
        "print(clssf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AlXvtek9wvTL",
        "outputId": "689649df-3b6b-4a58-ae3d-7da132d75b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.74      0.83       182\n",
            "           1       0.99      0.93      0.95       240\n",
            "           2       0.92      0.67      0.77       242\n",
            "           3       0.69      0.82      0.75       181\n",
            "           4       0.96      0.95      0.95       182\n",
            "           5       0.71      0.83      0.76       136\n",
            "           6       0.97      0.86      0.91       241\n",
            "           7       0.82      0.86      0.84       181\n",
            "           8       0.92      0.93      0.93       182\n",
            "           9       0.66      0.90      0.76       100\n",
            "          10       0.91      0.87      0.89       102\n",
            "          11       0.96      0.99      0.98       133\n",
            "          12       0.93      0.97      0.95       181\n",
            "          13       0.97      0.93      0.95       182\n",
            "          14       0.92      0.96      0.94       121\n",
            "          15       0.95      0.92      0.94       240\n",
            "          16       0.82      0.93      0.88       181\n",
            "          17       1.00      0.94      0.97       242\n",
            "          18       0.81      0.81      0.81       181\n",
            "          19       0.88      0.91      0.89       101\n",
            "          20       1.00      0.96      0.98       137\n",
            "          21       0.99      0.93      0.96       241\n",
            "          22       0.94      0.97      0.95       136\n",
            "          23       0.73      0.72      0.72       132\n",
            "          24       0.81      0.80      0.80       181\n",
            "          25       0.95      0.87      0.91       181\n",
            "          26       1.00      0.99      1.00       101\n",
            "          27       1.00      0.97      0.99       242\n",
            "          28       0.90      0.92      0.91       103\n",
            "          29       0.94      0.98      0.96       179\n",
            "          30       0.74      0.74      0.74       121\n",
            "          31       0.85      0.95      0.90       136\n",
            "          32       0.85      0.79      0.82       241\n",
            "          33       0.95      0.94      0.95       180\n",
            "          34       0.98      0.93      0.96       183\n",
            "          35       0.80      0.77      0.79       134\n",
            "          36       0.60      0.88      0.72       182\n",
            "          37       0.83      0.96      0.89       134\n",
            "          38       0.85      0.96      0.90       135\n",
            "          39       0.73      0.97      0.83       132\n",
            "          40       0.89      0.92      0.90       181\n",
            "          41       0.96      0.89      0.92       244\n",
            "          42       0.99      1.00      0.99       238\n",
            "          43       0.92      0.91      0.91       135\n",
            "          44       0.91      0.97      0.94       119\n",
            "          45       0.92      0.92      0.92       182\n",
            "          46       0.98      0.91      0.94       243\n",
            "          47       0.75      0.90      0.82       136\n",
            "          48       0.92      0.78      0.85       182\n",
            "          49       0.93      0.74      0.83       180\n",
            "          50       0.58      0.85      0.69       100\n",
            "          51       0.62      0.56      0.59       135\n",
            "          52       0.85      0.90      0.87       181\n",
            "          53       0.92      0.74      0.82       244\n",
            "          54       0.64      0.83      0.72       181\n",
            "          55       0.96      0.98      0.97       119\n",
            "          56       1.00      0.86      0.93       237\n",
            "          57       0.86      0.71      0.78       183\n",
            "          58       0.98      0.97      0.98       183\n",
            "          59       0.94      0.96      0.95       121\n",
            "          60       0.92      0.99      0.96       135\n",
            "          61       0.96      0.99      0.98       179\n",
            "          62       0.99      0.78      0.87       243\n",
            "          63       0.82      0.93      0.87       134\n",
            "          64       0.86      0.95      0.91       121\n",
            "          65       0.87      0.64      0.74       121\n",
            "          66       0.95      1.00      0.97       182\n",
            "          67       0.58      0.73      0.64       172\n",
            "          68       0.99      0.85      0.91       181\n",
            "          69       0.78      0.90      0.83       136\n",
            "          70       0.90      0.75      0.82       241\n",
            "          71       0.96      1.00      0.98       102\n",
            "          72       0.89      0.98      0.93       121\n",
            "          73       0.98      0.92      0.95       243\n",
            "          74       0.97      0.97      0.97       182\n",
            "          75       0.74      0.88      0.80       131\n",
            "          76       0.90      0.96      0.93       136\n",
            "          77       1.00      0.94      0.97       136\n",
            "          78       0.97      0.98      0.97       177\n",
            "          79       1.00      0.64      0.78       242\n",
            "          80       0.81      0.64      0.71       137\n",
            "          81       0.70      0.87      0.78       180\n",
            "          82       0.99      0.99      0.99       135\n",
            "          83       0.91      0.92      0.91       136\n",
            "          84       0.88      0.91      0.89       137\n",
            "          85       0.99      0.93      0.96       243\n",
            "          86       0.98      0.95      0.96       243\n",
            "          87       0.83      0.83      0.83       179\n",
            "          88       1.00      0.94      0.97       181\n",
            "          89       0.91      0.82      0.86       122\n",
            "          90       0.94      0.93      0.94       180\n",
            "          91       0.93      0.76      0.84       135\n",
            "          92       0.98      0.95      0.96       100\n",
            "          93       0.89      0.96      0.92       181\n",
            "          94       0.72      0.81      0.76       182\n",
            "          95       0.91      0.94      0.93       181\n",
            "          96       0.95      0.97      0.96       182\n",
            "          97       0.94      0.98      0.96       131\n",
            "          98       0.95      0.98      0.96       180\n",
            "          99       1.00      0.98      0.99       243\n",
            "         100       0.83      0.97      0.90       181\n",
            "         101       0.91      0.88      0.89       181\n",
            "         102       0.92      0.86      0.89       181\n",
            "         103       0.96      0.90      0.93       137\n",
            "         104       0.73      0.71      0.72       243\n",
            "         105       1.00      0.94      0.97       120\n",
            "         106       1.00      0.94      0.97       241\n",
            "         107       0.95      0.98      0.96       241\n",
            "         108       0.93      0.98      0.95       102\n",
            "         109       1.00      0.97      0.99       181\n",
            "         110       0.95      0.93      0.94       133\n",
            "         111       0.78      0.95      0.85       102\n",
            "         112       1.00      0.98      0.99       121\n",
            "         113       0.99      0.99      0.99       136\n",
            "         114       0.96      0.97      0.97       102\n",
            "         115       0.95      0.98      0.96       177\n",
            "         116       1.00      0.97      0.98       101\n",
            "         117       0.61      0.71      0.66       181\n",
            "         118       0.98      1.00      0.99       131\n",
            "         119       0.98      0.96      0.97       135\n",
            "         120       0.97      0.98      0.97       136\n",
            "         121       0.98      0.98      0.98       242\n",
            "         122       0.96      0.97      0.97       101\n",
            "         123       0.90      0.68      0.77       182\n",
            "         124       0.89      0.71      0.79       242\n",
            "         125       0.79      0.98      0.88       101\n",
            "         126       0.68      0.87      0.77       103\n",
            "         127       0.86      0.96      0.91       135\n",
            "         128       0.91      0.93      0.92       182\n",
            "         129       0.74      0.96      0.83       136\n",
            "         130       0.72      0.65      0.68       137\n",
            "         131       0.99      0.83      0.90       181\n",
            "         132       0.72      0.66      0.69       241\n",
            "         133       0.98      0.78      0.87       243\n",
            "         134       1.00      1.00      1.00       240\n",
            "         135       0.79      0.99      0.88       101\n",
            "         136       0.95      0.94      0.95       121\n",
            "         137       0.99      0.99      0.99       136\n",
            "         138       0.95      0.99      0.97       180\n",
            "         139       0.92      0.93      0.92       181\n",
            "         140       0.94      0.75      0.83       182\n",
            "         141       0.96      0.94      0.95       182\n",
            "         142       0.92      0.93      0.93       182\n",
            "         143       1.00      0.96      0.98       103\n",
            "         144       0.87      0.87      0.87       134\n",
            "         145       0.87      0.94      0.90       180\n",
            "         146       0.94      0.93      0.93       243\n",
            "         147       0.52      0.52      0.52       182\n",
            "         148       0.90      0.93      0.92       122\n",
            "         149       0.99      0.69      0.82       243\n",
            "         150       0.44      0.59      0.50       122\n",
            "         151       0.90      0.97      0.93       182\n",
            "         152       0.89      0.97      0.93       136\n",
            "         153       0.76      0.84      0.80       182\n",
            "         154       0.46      0.71      0.56       180\n",
            "         155       0.95      0.96      0.96       176\n",
            "         156       0.72      0.61      0.66       181\n",
            "         157       0.97      0.93      0.95       182\n",
            "         158       0.95      0.90      0.92       181\n",
            "         159       0.96      1.00      0.98       136\n",
            "         160       0.90      0.84      0.87       135\n",
            "         161       0.71      0.46      0.55       178\n",
            "         162       0.88      0.96      0.92       134\n",
            "         163       1.00      0.95      0.97       182\n",
            "         164       0.97      0.94      0.95       182\n",
            "         165       0.74      0.90      0.81       176\n",
            "         166       0.44      0.63      0.52       174\n",
            "         167       1.00      0.87      0.93       136\n",
            "         168       0.92      0.96      0.94       182\n",
            "         169       0.94      1.00      0.97       102\n",
            "         170       0.94      0.91      0.93       183\n",
            "         171       0.86      0.91      0.88       182\n",
            "         172       0.78      0.63      0.69       132\n",
            "         173       0.41      0.57      0.48       136\n",
            "         174       0.43      0.59      0.50       182\n",
            "         175       0.42      0.36      0.39       179\n",
            "         176       0.99      0.98      0.99       176\n",
            "         177       0.74      0.86      0.80       102\n",
            "         178       0.87      0.72      0.79       242\n",
            "         179       0.85      0.84      0.84       135\n",
            "         180       0.92      0.67      0.78       243\n",
            "         181       0.97      0.69      0.80       242\n",
            "         182       0.98      0.90      0.94       242\n",
            "         183       0.99      0.90      0.94       242\n",
            "         184       0.91      0.94      0.92       129\n",
            "         185       0.98      0.95      0.97       182\n",
            "         186       0.73      0.89      0.80       180\n",
            "         187       0.88      0.90      0.89       136\n",
            "         188       0.78      0.78      0.78       181\n",
            "         189       0.81      0.96      0.87       134\n",
            "         190       0.74      0.81      0.77       134\n",
            "         191       0.90      0.98      0.94       131\n",
            "         192       0.75      0.82      0.78       182\n",
            "         193       0.87      0.85      0.86       137\n",
            "         194       0.75      0.87      0.81       243\n",
            "         195       0.91      0.89      0.90       121\n",
            "         196       0.91      0.92      0.92       181\n",
            "         197       1.00      0.99      0.99       137\n",
            "         198       0.86      0.95      0.90       120\n",
            "         199       1.00      0.89      0.94       244\n",
            "         200       0.80      0.98      0.88       101\n",
            "\n",
            "    accuracy                           0.87     33700\n",
            "   macro avg       0.88      0.88      0.87     33700\n",
            "weighted avg       0.88      0.87      0.87     33700\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k59_FbfSwzDN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}