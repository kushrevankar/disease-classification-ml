{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy scikit-learn iterative-stratification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1-DySsSnOI-",
        "outputId": "f9c7b7df-65fb-4539-9608-89045b4236d7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, iterative-stratification\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed iterative-stratification-0.1.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv40CASPoPuF",
        "outputId": "a888c594-fb7b-4f7f-9a3b-61981efd7120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "cTaF4PqrnUEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/diseasedataset/properfinal.csv\"\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "13Y4hINrouRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode diseases column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(df[[\"diseases\"]])\n",
        "\n",
        "# split into features & labels\n",
        "X = df.drop(columns=[\"diseases\"]).values\n",
        "y = y_encoded"
      ],
      "metadata": {
        "id": "NmTjPKPwo65o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-dtzem6mbpX",
        "outputId": "e1ac07c9-b867-4ef6-faac-cddc1e7faf1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 210623 entries, 0 to 210622\n",
            "Columns: 379 entries, Unnamed: 0 to diseases\n",
            "dtypes: int64(378), object(1)\n",
            "memory usage: 609.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# splitting into train-test (one iteration only)\n",
        "for train_index, test_index in mskf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    break  # only take the first split"
      ],
      "metadata": {
        "id": "X6nEbuJ6pIzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HDCClassifier:\n",
        "    def __init__(self, dim=1000):\n",
        "        self.dim = dim  # Hyperdimensional vector size\n",
        "        self.prototypes = {}  # Stores prototype vectors per class\n",
        "\n",
        "    def _encode(self, x):\n",
        "        \"\"\" Convert input features into hyperdimensional binary vectors. \"\"\"\n",
        "        return torch.sign(torch.tensor(x, dtype=torch.float32))  # Convert to float\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        \"\"\" Train by averaging class-specific hyperdimensional vectors. \"\"\"\n",
        "        for label in np.unique(y_train):\n",
        "            class_vectors = [self._encode(X_train[i]) for i in range(len(y_train)) if y_train[i] == label]\n",
        "            self.prototypes[label] = torch.mean(torch.stack(class_vectors), dim=0).float()  # Ensure float type\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\" Predict based on similarity to stored class prototypes. \"\"\"\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            encoded_x = self._encode(x)\n",
        "            similarities = {label: torch.dot(encoded_x, proto) for label, proto in self.prototypes.items()}\n",
        "            predictions.append(max(similarities, key=similarities.get))  # Closest class\n",
        "        return np.array(predictions)"
      ],
      "metadata": {
        "id": "WIJh4DoHpe2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_labels = y_train.argmax(axis=1)  # Convert from one-hot to single-label vector\n",
        "y_test_labels = y_test.argmax(axis=1)"
      ],
      "metadata": {
        "id": "OeLHd7jnxUy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize & train\n",
        "model = HDCClassifier()\n",
        "model.fit(X_train, y_train_labels)"
      ],
      "metadata": {
        "id": "UNviPAwsppNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "XxqIayAIprK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test_labels, y_pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "panS3nMhqLPh",
        "outputId": "97e6688e-9013-4bb1-9d28-384fe508e9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8618364827651696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_test_labels, y_pred, average='macro', zero_division=1)\n",
        "print(\"Precision:\", precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zdojMCusHNO",
        "outputId": "a5b2421f-571f-4d5e-af76-19cf4c1907a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8653660324290485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall = recall_score(y_test_labels, y_pred, average='macro', zero_division=1)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23_NC5U33Vvb",
        "outputId": "198b3937-f97d-4764-848c-071b508b2bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.8687788396700531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clssf = classification_report(y_test_labels, y_pred)\n",
        "print(clssf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AlXvtek9wvTL",
        "outputId": "6412d667-6dbe-4982-a5a0-f029fdc61909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.74      0.82       228\n",
            "           1       0.98      0.91      0.94       302\n",
            "           2       0.86      0.65      0.74       311\n",
            "           3       0.64      0.76      0.69       228\n",
            "           4       0.94      0.96      0.95       232\n",
            "           5       0.73      0.84      0.78       170\n",
            "           6       0.99      0.87      0.93       303\n",
            "           7       0.86      0.90      0.88       223\n",
            "           8       0.92      0.91      0.91       228\n",
            "           9       0.60      0.94      0.73       127\n",
            "          10       0.88      0.92      0.90       128\n",
            "          11       0.94      0.98      0.96       167\n",
            "          12       0.91      0.97      0.94       227\n",
            "          13       0.97      0.92      0.94       224\n",
            "          14       0.91      0.89      0.90       150\n",
            "          15       0.94      0.93      0.93       299\n",
            "          16       0.83      0.95      0.88       233\n",
            "          17       0.97      0.94      0.95       300\n",
            "          18       0.77      0.79      0.78       224\n",
            "          19       0.87      0.91      0.89       128\n",
            "          20       0.99      0.94      0.96       171\n",
            "          21       0.97      0.96      0.97       304\n",
            "          22       0.87      0.95      0.91       172\n",
            "          23       0.58      0.69      0.63       170\n",
            "          24       0.80      0.74      0.77       226\n",
            "          25       0.92      0.89      0.91       226\n",
            "          26       0.99      0.98      0.99       124\n",
            "          27       0.99      0.98      0.99       297\n",
            "          28       0.89      0.84      0.87       129\n",
            "          29       0.96      0.98      0.97       225\n",
            "          30       0.76      0.76      0.76       153\n",
            "          31       0.85      0.95      0.90       170\n",
            "          32       0.95      0.74      0.83       301\n",
            "          33       0.95      0.96      0.96       224\n",
            "          34       0.96      0.88      0.92       235\n",
            "          35       0.76      0.76      0.76       169\n",
            "          36       0.58      0.83      0.68       223\n",
            "          37       0.87      0.94      0.90       167\n",
            "          38       0.78      0.93      0.85       165\n",
            "          39       0.82      0.95      0.88       163\n",
            "          40       0.86      0.89      0.87       226\n",
            "          41       0.93      0.85      0.89       299\n",
            "          42       0.99      0.98      0.99       302\n",
            "          43       0.91      0.92      0.92       171\n",
            "          44       0.88      0.94      0.91       151\n",
            "          45       0.88      0.88      0.88       230\n",
            "          46       0.93      0.87      0.90       303\n",
            "          47       0.78      0.83      0.81       169\n",
            "          48       0.91      0.72      0.80       228\n",
            "          49       0.91      0.77      0.83       227\n",
            "          50       0.51      0.85      0.64       121\n",
            "          51       0.68      0.62      0.65       164\n",
            "          52       0.77      0.89      0.83       219\n",
            "          53       0.93      0.68      0.78       303\n",
            "          54       0.61      0.79      0.69       228\n",
            "          55       0.97      0.96      0.97       147\n",
            "          56       0.98      0.89      0.93       298\n",
            "          57       0.88      0.71      0.79       226\n",
            "          58       0.98      0.96      0.97       232\n",
            "          59       0.94      0.98      0.96       152\n",
            "          60       0.87      0.96      0.91       169\n",
            "          61       0.97      0.97      0.97       224\n",
            "          62       0.98      0.81      0.89       300\n",
            "          63       0.81      0.91      0.86       167\n",
            "          64       0.92      0.90      0.91       153\n",
            "          65       0.87      0.72      0.79       149\n",
            "          66       0.92      0.98      0.95       227\n",
            "          67       0.61      0.70      0.65       215\n",
            "          68       0.94      0.88      0.91       226\n",
            "          69       0.85      0.84      0.85       174\n",
            "          70       0.89      0.72      0.79       300\n",
            "          71       0.97      0.99      0.98       127\n",
            "          72       0.94      0.95      0.95       152\n",
            "          73       0.96      0.91      0.94       302\n",
            "          74       0.96      0.95      0.96       226\n",
            "          75       0.65      0.84      0.73       161\n",
            "          76       0.89      0.91      0.90       166\n",
            "          77       0.97      0.95      0.96       170\n",
            "          78       0.95      0.99      0.97       219\n",
            "          79       0.98      0.64      0.77       306\n",
            "          80       0.72      0.76      0.74       172\n",
            "          81       0.69      0.90      0.78       222\n",
            "          82       0.97      0.98      0.98       169\n",
            "          83       0.82      0.86      0.84       170\n",
            "          84       0.85      0.89      0.87       171\n",
            "          85       0.96      0.89      0.93       303\n",
            "          86       0.98      0.96      0.97       308\n",
            "          87       0.83      0.84      0.84       221\n",
            "          88       0.97      0.92      0.95       226\n",
            "          89       0.92      0.84      0.88       153\n",
            "          90       0.94      0.95      0.94       225\n",
            "          91       0.84      0.76      0.80       169\n",
            "          92       0.90      0.94      0.92       126\n",
            "          93       0.93      0.89      0.91       228\n",
            "          94       0.73      0.78      0.76       231\n",
            "          95       0.93      0.94      0.93       231\n",
            "          96       0.91      0.93      0.92       230\n",
            "          97       0.91      0.98      0.94       164\n",
            "          98       0.96      0.96      0.96       224\n",
            "          99       1.00      0.97      0.98       309\n",
            "         100       0.84      0.92      0.88       226\n",
            "         101       0.88      0.89      0.89       228\n",
            "         102       0.91      0.87      0.89       220\n",
            "         103       0.97      0.91      0.94       171\n",
            "         104       0.78      0.65      0.71       300\n",
            "         105       0.98      0.95      0.97       148\n",
            "         106       0.99      0.95      0.97       299\n",
            "         107       0.97      0.97      0.97       299\n",
            "         108       0.95      0.98      0.97       128\n",
            "         109       0.94      0.94      0.94       221\n",
            "         110       0.90      0.95      0.92       165\n",
            "         111       0.79      0.87      0.83       127\n",
            "         112       0.96      0.95      0.95       152\n",
            "         113       1.00      0.98      0.99       171\n",
            "         114       0.95      0.98      0.96       127\n",
            "         115       0.96      0.98      0.97       221\n",
            "         116       0.98      0.94      0.96       124\n",
            "         117       0.67      0.76      0.71       229\n",
            "         118       0.95      0.99      0.97       162\n",
            "         119       0.96      0.94      0.95       171\n",
            "         120       0.92      0.98      0.95       169\n",
            "         121       0.97      0.95      0.96       309\n",
            "         122       0.97      0.96      0.96       124\n",
            "         123       0.90      0.71      0.79       223\n",
            "         124       0.89      0.63      0.74       303\n",
            "         125       0.83      0.95      0.88       128\n",
            "         126       0.67      0.87      0.76       130\n",
            "         127       0.89      0.94      0.91       169\n",
            "         128       0.91      0.92      0.91       230\n",
            "         129       0.73      0.92      0.82       172\n",
            "         130       0.57      0.70      0.63       170\n",
            "         131       0.94      0.81      0.87       223\n",
            "         132       0.70      0.74      0.72       302\n",
            "         133       0.98      0.82      0.89       305\n",
            "         134       0.99      0.99      0.99       302\n",
            "         135       0.71      0.98      0.83       124\n",
            "         136       0.92      0.97      0.95       152\n",
            "         137       0.98      0.97      0.97       173\n",
            "         138       0.95      0.98      0.96       221\n",
            "         139       0.91      0.94      0.93       224\n",
            "         140       0.90      0.80      0.85       229\n",
            "         141       0.89      0.90      0.89       222\n",
            "         142       0.91      0.93      0.92       231\n",
            "         143       0.95      0.93      0.94       128\n",
            "         144       0.83      0.87      0.85       167\n",
            "         145       0.85      0.88      0.87       229\n",
            "         146       0.96      0.90      0.93       304\n",
            "         147       0.50      0.61      0.55       227\n",
            "         148       0.91      0.93      0.92       150\n",
            "         149       0.98      0.69      0.81       303\n",
            "         150       0.49      0.48      0.49       151\n",
            "         151       0.85      0.96      0.90       227\n",
            "         152       0.85      0.96      0.90       168\n",
            "         153       0.64      0.83      0.72       229\n",
            "         154       0.49      0.53      0.51       230\n",
            "         155       0.96      0.94      0.95       214\n",
            "         156       0.70      0.68      0.69       224\n",
            "         157       0.98      0.93      0.95       230\n",
            "         158       0.85      0.91      0.88       225\n",
            "         159       0.99      0.99      0.99       170\n",
            "         160       0.90      0.77      0.83       168\n",
            "         161       0.59      0.60      0.59       226\n",
            "         162       0.92      0.93      0.93       165\n",
            "         163       0.96      0.96      0.96       228\n",
            "         164       0.98      0.87      0.92       227\n",
            "         165       0.79      0.83      0.81       219\n",
            "         166       0.44      0.60      0.51       213\n",
            "         167       0.97      0.87      0.92       172\n",
            "         168       0.92      0.97      0.94       230\n",
            "         169       0.96      1.00      0.98       127\n",
            "         170       0.96      0.89      0.92       226\n",
            "         171       0.85      0.88      0.87       226\n",
            "         172       0.74      0.66      0.70       163\n",
            "         173       0.41      0.69      0.51       172\n",
            "         174       0.53      0.48      0.51       229\n",
            "         175       0.38      0.37      0.38       226\n",
            "         176       0.99      1.00      0.99       218\n",
            "         177       0.74      0.75      0.75       126\n",
            "         178       0.86      0.64      0.73       300\n",
            "         179       0.79      0.87      0.83       168\n",
            "         180       0.94      0.57      0.71       304\n",
            "         181       0.91      0.75      0.82       301\n",
            "         182       0.98      0.85      0.91       307\n",
            "         183       0.96      0.89      0.92       305\n",
            "         184       0.89      0.98      0.93       161\n",
            "         185       0.98      0.92      0.95       230\n",
            "         186       0.75      0.88      0.81       227\n",
            "         187       0.87      0.94      0.91       174\n",
            "         188       0.78      0.81      0.79       223\n",
            "         189       0.86      0.94      0.90       171\n",
            "         190       0.77      0.83      0.80       167\n",
            "         191       0.89      0.94      0.92       163\n",
            "         192       0.78      0.81      0.79       235\n",
            "         193       0.87      0.87      0.87       173\n",
            "         194       0.79      0.84      0.81       301\n",
            "         195       0.89      0.91      0.90       153\n",
            "         196       0.92      0.90      0.91       227\n",
            "         197       1.00      0.99      0.99       167\n",
            "         198       0.90      0.93      0.91       148\n",
            "         199       0.95      0.88      0.91       307\n",
            "         200       0.81      0.98      0.89       124\n",
            "\n",
            "    accuracy                           0.86     42124\n",
            "   macro avg       0.87      0.87      0.86     42124\n",
            "weighted avg       0.87      0.86      0.86     42124\n",
            "\n"
          ]
        }
      ]
    }
  ]
}